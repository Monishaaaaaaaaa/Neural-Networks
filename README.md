# Artificial Neural Network with Two Hidden Layers

## Project Overview

This project implements an Artificial Neural Network (ANN) with two hidden layers for supervised learning tasks. The objective is to model complex nonlinear relationships between input features and the target variable using deep learning techniques.

The model is trained using forward propagation and optimized using backpropagation with gradient-based optimization.

---

## Problem Statement

The goal of this project is to build a multi-layer neural network capable of learning complex feature interactions and improving prediction accuracy compared to traditional machine learning algorithms.

---

## Model Architecture

The neural network consists of:

- Input Layer
- Hidden Layer 1 
- Hidden Layer 2 
- Output Layer

Activation Functions:
- ReLU (Hidden Layers)
- Sigmoid / Softmax (Output Layer, depending on task)

---

## Methodology

1. Data preprocessing and feature scaling
2. Model architecture design with two hidden layers
3. Forward propagation
4. Backpropagation for weight updates
5. Model training and validation
6. Performance evaluation

---

## Evaluation Metrics

- Accuracy
- Loss
- Precision
- Recall
- F1 Score

---

## Key Concepts Demonstrated

- Multi-layer neural networks
- Nonlinear activation functions
- Backpropagation algorithm
- Overfitting control techniques 
- Model optimization

---

## Technologies Used

- Python
- TensorFlow / Keras
- NumPy
- Pandas
- Matplotlib

---

## Conclusion

This project demonstrates the implementation of a deep feedforward neural network with two hidden layers, capable of learning complex patterns in structured data and improving predictive performance compared to shallow models.
